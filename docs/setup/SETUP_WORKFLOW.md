# ğŸš€ Setup Workflow

**âš ï¸ Note:** This is an older workflow guide. For the most up-to-date setup instructions, see **[GETTING_STARTED.md](../GETTING_STARTED.md)**.

---

## âœ… Recommended Setup (2026 Method)

**Use the automated startup script:**

```bash
# 1. Create .env file
cp .env.example .env
nano .env  # Add your FINNHUB_API_KEY

# 2. Run the startup script
./start_data_pipeline.sh
```

**That's it!** The script handles everything automatically.

See [GETTING_STARTED.md](../GETTING_STARTED.md) for detailed instructions.

---

## Alternative Setup Methods

### Method 1: Docker Compose with Profiles (Manual)

```bash
# 1. Setup environment variables
cp .env.example .env
# Edit .env and add FINNHUB_API_KEY

# 2. Start all services (infrastructure + producers)
docker-compose --profile producers up -d

# 3. Wait for services to be ready (30-60 seconds)
sleep 60

# 4. Submit Flink job manually
docker exec market_jobmanager ./bin/flink run -py /opt/flink/usrlib/flink_sentiment.py

# 5. Access dashboard
# http://localhost:8502
```

### Method 2: Local Development + Docker

```bash
# 1. Create virtual environment (optional, for local development)
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or: .venv\Scripts\activate  # Windows

# 2. Install dependencies (optional, for local development)
pip install -r requirements.txt

# 3. Setup .env
cp .env.example .env
# Edit and add FINNHUB_API_KEY

# 4. Start Docker containers
docker-compose --profile producers up -d

# 5. Submit Flink job
docker exec market_jobmanager ./bin/flink run -py /opt/flink/usrlib/flink_sentiment.py
```

---

## âš ï¸ Original UV Workflow (Deprecated)

**Note:** UV setup is no longer required. The Docker containers handle all dependencies.

<details>
<summary>Click to view original UV workflow (for reference only)</summary>

### Step 1: Create Virtual Environment with UV

```bash
# Install UV (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment
uv venv

# Activate virtual environment
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate     # Windows
```

### Step 2: Install Python Packages

```bash
# Install all dependencies from requirements.txt
uv pip install -r requirements.txt

# Verify installation
pip list
```

### Step 3: Setup Environment Variables

```bash
# Create .env file
cat > .env << EOF
FINNHUB_API_KEY=your_finnhub_api_key_here
STOCK_SYMBOLS=AAPL,MSFT,TSLA,GOOGL,AMZN
EOF
```

### Step 4: Start Docker Containers

```bash
# Build and start all services
docker-compose up -d --build
```

### Step 5: Run Producers

```bash
# Terminal 1 - News Producer
docker-compose run --rm producer python news_producer.py

# Terminal 2 - Price Producer
docker-compose run --rm producer python price_producer.py

# Terminal 3 - Price Consumer
docker-compose run --rm producer python price_consumer.py
```

</details>

---

## Why Use the Startup Script?

### âœ… Advantages of `./start_data_pipeline.sh`:
- **Automated** - All steps handled for you
- **Validation** - Checks .env, files, Docker status
- **Sequencing** - Waits for services to be healthy
- **Error Handling** - Clear error messages
- **Flink Job** - Automatically submitted
- **Status Reporting** - Shows what's running

### âš ï¸ Manual Docker Compose Issues:
- No pre-flight checks
- Race conditions (producers before Kafka ready)
- Manual Flink job submission required
- No IP detection for Ollama
- Generic error messages

See [DOCKER_VS_SCRIPT_GUIDE.md](../DOCKER_VS_SCRIPT_GUIDE.md) for detailed comparison.

---

## Development Workflow

### Local Testing (Optional)

If you want to test scripts locally before Docker:

```bash
# 1. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Test locally (requires Kafka running)
python producer/news_producer.py
python producer/price_producer.py

# 4. Run dashboard locally
streamlit run dashboard/app.py
```

### Docker Deployment

```bash
# Use the startup script (recommended)
./start_data_pipeline.sh

# OR manual docker-compose (see Method 1 above)
```

---

## File Structure After Setup

```
Market_Mood_Ring/
â”œâ”€â”€ .env                      # Your environment variables (created)
â”œâ”€â”€ .env.example              # Template
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ start_data_pipeline.sh    # â­ Startup script (use this!)
â”œâ”€â”€ docker-compose.yaml       # Docker orchestration
â”‚
â”œâ”€â”€ producer/                 # Data ingestion
â”‚   â”œâ”€â”€ news_producer.py
â”‚   â”œâ”€â”€ price_producer.py
â”‚   â”œâ”€â”€ price_consumer.py
â”‚   â””â”€â”€ rag_ingest.py
â”‚
â”œâ”€â”€ flink_jobs/               # Stream processing
â”‚   â””â”€â”€ flink_sentiment.py
â”‚
â”œâ”€â”€ dashboard/                # UI
â”‚   â””â”€â”€ app.py
â”‚
â””â”€â”€ docs/                     # Documentation
    â”œâ”€â”€ GETTING_STARTED.md    # â­ Modern guide
    â””â”€â”€ ...
```

---

## Summary

**Recommended Approach (2026):**
1. âœ… Create `.env` with FINNHUB_API_KEY
2. âœ… Run `./start_data_pipeline.sh`
3. âœ… Access dashboard at http://localhost:8502

**Benefits:**
- âœ… Fast setup (2-3 minutes)
- âœ… Automated validation
- âœ… Proper service sequencing
- âœ… Clear status reporting

**For detailed walkthrough:** [GETTING_STARTED.md](../GETTING_STARTED.md)  
**For troubleshooting:** [TROUBLESHOOTING.md](../TROUBLESHOOTING.md)

---

*Last Updated: February 2026*  
*Status: Modernized with startup script*
